{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%file test_lab_3.py\n",
        "\n",
        "\n",
        "from torch.nn.functional import conv_transpose2d\n",
        "import pytest\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "def generate_zeros(input, weight, stride, padding, output_padding):\n",
        "    batch_size, in_channels, in_height, in_width = input.shape\n",
        "    out_channels, in_channels, kernel_height, kernel_width = weight.shape\n",
        "    H_out = (in_height - 1) * stride - 2 * padding + kernel_height + output_padding\n",
        "    W_out = (in_width - 1) * stride - 2 * padding + kernel_width + output_padding\n",
        "    output = np.zeros((batch_size, out_channels, H_out, W_out))\n",
        "    return (batch_size, out_channels, H_out, W_out,\n",
        "            in_channels, kernel_height,\n",
        "            kernel_width, in_height, in_width, output)\n",
        "\n",
        "def ConvolutionTranspose(input_data, weight_tensor, padding=0, dilation=1, stride=1, groups = 1):\n",
        "    (batch_size, out_channels, H_out,\n",
        "    W_out, in_channels,\n",
        "    kernel_height, kernel_width,\n",
        "    in_height, in_width, result) = generate_zeros(input_data, weight_tensor,\n",
        "                                                       stride, padding,\n",
        "                                                       padding)\n",
        "\n",
        "    if padding > 0:\n",
        "        input_data = np.pad(input_data, padding, mode='constant')\n",
        "\n",
        "    result = np.zeros((batch_size, out_channels, H_out, W_out))\n",
        "\n",
        "    for b in range(batch_size):\n",
        "        for c in range(out_channels):\n",
        "            for i in range(H_out):\n",
        "                for j in range(W_out):\n",
        "                    for k in range(in_channels):\n",
        "                        for s in range(kernel_height):\n",
        "                            for t in range(kernel_width):\n",
        "                                ii = i + padding - s * dilation\n",
        "                                jj = j + padding - t * dilation\n",
        "                                if ii >= 0 and jj >= 0 and ii < in_height * stride and jj < in_width * stride and (ii % stride == 0) and (jj % stride == 0):\n",
        "                                    ii //= stride\n",
        "                                    jj //= stride\n",
        "                                    result[b, c, i, j] += np.multiply(input_data[b, k, ii, jj], weight_tensor[c, k, s, t])\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_1():\n",
        "    image = torch.randn(1, 1, 3, 3)\n",
        "    weight = torch.randn(1, 1, 3, 3)\n",
        "\n",
        "    myConvT = torch.from_numpy(ConvolutionTranspose(image.numpy(), weight.numpy()))\n",
        "\n",
        "    torchConvT = conv_transpose2d(image, weight)\n",
        "\n",
        "    myConvT = myConvT.to(torchConvT.dtype)\n",
        "\n",
        "    assert torch.allclose(myConvT, torchConvT)\n",
        "\n",
        "\n",
        "def test_2():\n",
        "    image = torch.randn(1, 1, 5, 5)\n",
        "    weight = torch.randn(1, 1, 3, 3)\n",
        "\n",
        "    myConvT = torch.from_numpy(ConvolutionTranspose(image.numpy(), weight.numpy()))\n",
        "\n",
        "    torchConvT = conv_transpose2d(image, weight)\n",
        "\n",
        "    myConvT = myConvT.to(torchConvT.dtype)\n",
        "\n",
        "    assert torch.allclose(myConvT, torchConvT)\n",
        "\n",
        "def test_3():\n",
        "    image = torch.randn(1, 1, 5, 5)\n",
        "    weight = torch.randn(1, 1, 3, 3)\n",
        "\n",
        "    myConvT = torch.from_numpy(ConvolutionTranspose(image.numpy(), weight.numpy()))\n",
        "\n",
        "    torchConvT = conv_transpose2d(image, weight)\n",
        "\n",
        "    myConvT = myConvT.to(torchConvT.dtype)\n",
        "\n",
        "    assert torch.allclose(myConvT, torchConvT)\n",
        "\n",
        "def test_4():\n",
        "    image = torch.randn(1, 1, 7, 7)\n",
        "    weight = torch.randn(1, 1, 3, 3)\n",
        "\n",
        "    myConvT = torch.from_numpy(ConvolutionTranspose(image.numpy(), weight.numpy()))\n",
        "\n",
        "    torchConvT = conv_transpose2d(image, weight)\n",
        "\n",
        "    myConvT = myConvT.to(torchConvT.dtype)\n",
        "\n",
        "    assert torch.allclose(myConvT, torchConvT)\n",
        "\n",
        "def test_5():\n",
        "    image = torch.randn(1, 1, 9, 9)\n",
        "    weight = torch.randn(1, 1, 5, 5)\n",
        "\n",
        "    myConvT = torch.from_numpy(ConvolutionTranspose(image.numpy(), weight.numpy()))\n",
        "\n",
        "    torchConvT = conv_transpose2d(image, weight)\n",
        "\n",
        "    myConvT = myConvT.to(torchConvT.dtype)\n",
        "\n",
        "    assert torch.allclose(myConvT, torchConvT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPnDGrSOM9yt",
        "outputId": "2c8e5093-64b0-408e-db2f-6077a406d07b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test_lab_3.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2sGHvdw653_",
        "outputId": "ee5731d0-4d69-4d84-ffa0-28754a3f96ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.4.3, pluggy-1.3.0\n",
            "rootdir: /content\n",
            "plugins: anyio-3.7.1\n",
            "collected 5 items                                                                                  \u001b[0m\n",
            "\n",
            "test_lab_3.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                          [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m5 passed\u001b[0m\u001b[32m in 4.13s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = torch.randn(1, 1, 9, 9)\n",
        "weight = torch.randn(1, 1, 5, 5)\n",
        "\n",
        "myConvT = torch.from_numpy(ConvolutionTranspose(image.numpy(), weight.numpy()))\n",
        "print(\"Результат выполнения моей функции ConvolutionTranspose:\")\n",
        "print(myConvT)\n",
        "\n",
        "torchConvT = conv_transpose2d(image, weight)\n",
        "print(\"Результат выполнения встроенной функции conv_transpose2d из библиотеки PyTorch:\")\n",
        "print(torchConvT)\n",
        "\n",
        "myConvT = myConvT.to(torchConvT.dtype)\n",
        "\n",
        "\n",
        "torch.allclose(myConvT, torchConvT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6RM4usgsUcM",
        "outputId": "b4d3543e-55c4-4490-f28f-b25c3e03b1ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат выполнения моей функции ConvolutionTranspose:\n",
            "tensor([[[[ 9.4909e-01, -5.7451e-01,  8.9306e-01, -4.4602e-01, -6.0922e-01,\n",
            "            5.4444e-01, -1.8029e+00, -1.2936e+00,  3.4492e-01,  1.4551e-01,\n",
            "           -1.4501e-01, -9.6834e-02, -2.2440e-02],\n",
            "          [-1.1759e-02, -1.9948e+00,  1.5880e+00,  1.2745e-01,  2.5963e+00,\n",
            "           -2.5192e+00,  9.7891e-01,  1.0645e+00,  9.9230e-01,  1.5090e+00,\n",
            "           -5.6474e-01,  5.3950e-02,  2.6525e-01],\n",
            "          [ 7.4824e-02, -2.9715e+00, -2.1657e+00,  4.8301e+00,  3.0426e+00,\n",
            "           -2.9415e+00,  8.1808e-01, -3.5283e+00, -1.0764e+00, -1.5699e+00,\n",
            "            2.5184e+00,  1.3477e+00, -4.3205e-01],\n",
            "          [-3.0911e+00,  2.4124e+00,  5.6406e+00,  8.7986e-01,  2.2736e+00,\n",
            "            4.0089e+00, -4.3301e+00,  5.8070e+00, -7.3173e-01,  1.1121e+00,\n",
            "            1.7933e+00, -1.3215e+00, -1.5420e+00],\n",
            "          [-1.9157e+00,  1.5963e+00,  1.5566e+00, -3.1425e+00, -1.0240e+01,\n",
            "            4.0715e+00,  1.9596e+00, -3.0293e+00,  2.1712e+00, -2.4746e-01,\n",
            "           -6.4070e+00, -5.3490e-01,  2.0691e+00],\n",
            "          [ 2.2009e+00, -7.5933e-01, -8.7131e+00, -1.1950e+00,  5.4811e-01,\n",
            "            3.8574e-01,  8.8520e+00, -7.9018e-01, -3.2337e+00, -4.0758e+00,\n",
            "           -2.9077e+00, -6.1835e-02,  4.7593e+00],\n",
            "          [-4.1461e+00, -2.2364e+00, -3.5619e+00, -5.3192e+00,  1.1639e+01,\n",
            "           -5.1577e+00, -1.2230e+01,  3.4381e+00, -1.2250e+00, -1.7599e+00,\n",
            "            6.3235e+00,  5.0917e-01, -6.5811e+00],\n",
            "          [-1.8437e+00, -2.2871e+00,  5.9456e+00,  8.5124e+00, -6.4749e+00,\n",
            "            3.7753e+00,  8.8133e-01, -4.3187e+00,  7.1808e+00,  7.4153e+00,\n",
            "            3.5502e+00,  2.0736e+00, -1.5457e-01],\n",
            "          [-3.5744e+00,  1.1650e+00,  2.7909e+00,  2.4726e+00, -2.9554e+00,\n",
            "           -6.9796e+00,  5.4996e+00,  1.8264e+00, -5.5544e+00,  7.6398e+00,\n",
            "            1.6429e-01, -5.0345e+00,  1.1360e+00],\n",
            "          [-1.1542e+00, -7.7906e-01, -2.0522e+00,  8.2946e-01,  2.2191e+00,\n",
            "            6.3192e-01, -2.4893e+00,  3.6487e-01,  7.6200e+00, -5.0275e+00,\n",
            "            8.0094e-01,  2.9365e+00, -1.5024e+00],\n",
            "          [-7.1895e-02, -5.1379e+00, -3.5215e+00, -2.0741e+00,  1.9583e+00,\n",
            "           -2.4844e+00, -7.4374e+00, -3.1537e+00, -2.0675e+00,  1.1589e-01,\n",
            "           -2.5268e+00, -2.7808e-01,  3.4432e+00],\n",
            "          [-9.5598e-01, -5.6541e+00, -4.3398e+00,  2.1750e-01,  3.3758e+00,\n",
            "           -6.1343e+00, -4.4104e+00, -6.2147e+00, -7.7017e+00, -1.1325e+00,\n",
            "           -8.7977e-01,  3.4523e+00, -1.1270e+00],\n",
            "          [ 1.0055e+00, -2.8595e+00, -4.5339e-01,  1.1271e+00, -1.2004e+00,\n",
            "            1.6971e+00, -2.8537e+00, -1.4042e+00, -7.0383e-01, -1.2624e+00,\n",
            "            6.3965e-01, -7.5155e-01,  9.4575e-02]]]], dtype=torch.float64)\n",
            "Результат выполнения встроенной функции conv_transpose2d из библиотеки PyTorch:\n",
            "tensor([[[[ 9.4909e-01, -5.7451e-01,  8.9306e-01, -4.4602e-01, -6.0922e-01,\n",
            "            5.4444e-01, -1.8029e+00, -1.2936e+00,  3.4492e-01,  1.4551e-01,\n",
            "           -1.4501e-01, -9.6834e-02, -2.2440e-02],\n",
            "          [-1.1759e-02, -1.9948e+00,  1.5880e+00,  1.2745e-01,  2.5963e+00,\n",
            "           -2.5192e+00,  9.7891e-01,  1.0645e+00,  9.9230e-01,  1.5090e+00,\n",
            "           -5.6474e-01,  5.3950e-02,  2.6525e-01],\n",
            "          [ 7.4824e-02, -2.9715e+00, -2.1657e+00,  4.8301e+00,  3.0426e+00,\n",
            "           -2.9415e+00,  8.1808e-01, -3.5283e+00, -1.0764e+00, -1.5699e+00,\n",
            "            2.5184e+00,  1.3477e+00, -4.3205e-01],\n",
            "          [-3.0911e+00,  2.4124e+00,  5.6406e+00,  8.7986e-01,  2.2736e+00,\n",
            "            4.0089e+00, -4.3301e+00,  5.8070e+00, -7.3173e-01,  1.1121e+00,\n",
            "            1.7933e+00, -1.3215e+00, -1.5420e+00],\n",
            "          [-1.9157e+00,  1.5963e+00,  1.5566e+00, -3.1425e+00, -1.0240e+01,\n",
            "            4.0715e+00,  1.9596e+00, -3.0293e+00,  2.1712e+00, -2.4746e-01,\n",
            "           -6.4070e+00, -5.3490e-01,  2.0691e+00],\n",
            "          [ 2.2009e+00, -7.5933e-01, -8.7131e+00, -1.1950e+00,  5.4811e-01,\n",
            "            3.8574e-01,  8.8520e+00, -7.9018e-01, -3.2337e+00, -4.0758e+00,\n",
            "           -2.9077e+00, -6.1835e-02,  4.7593e+00],\n",
            "          [-4.1461e+00, -2.2364e+00, -3.5619e+00, -5.3192e+00,  1.1639e+01,\n",
            "           -5.1577e+00, -1.2230e+01,  3.4381e+00, -1.2250e+00, -1.7599e+00,\n",
            "            6.3235e+00,  5.0917e-01, -6.5811e+00],\n",
            "          [-1.8437e+00, -2.2871e+00,  5.9456e+00,  8.5124e+00, -6.4749e+00,\n",
            "            3.7753e+00,  8.8133e-01, -4.3187e+00,  7.1808e+00,  7.4153e+00,\n",
            "            3.5502e+00,  2.0736e+00, -1.5457e-01],\n",
            "          [-3.5744e+00,  1.1650e+00,  2.7909e+00,  2.4726e+00, -2.9554e+00,\n",
            "           -6.9796e+00,  5.4996e+00,  1.8264e+00, -5.5544e+00,  7.6398e+00,\n",
            "            1.6429e-01, -5.0345e+00,  1.1360e+00],\n",
            "          [-1.1542e+00, -7.7906e-01, -2.0522e+00,  8.2946e-01,  2.2191e+00,\n",
            "            6.3192e-01, -2.4893e+00,  3.6487e-01,  7.6200e+00, -5.0275e+00,\n",
            "            8.0094e-01,  2.9365e+00, -1.5024e+00],\n",
            "          [-7.1895e-02, -5.1379e+00, -3.5215e+00, -2.0741e+00,  1.9583e+00,\n",
            "           -2.4844e+00, -7.4374e+00, -3.1537e+00, -2.0675e+00,  1.1589e-01,\n",
            "           -2.5268e+00, -2.7808e-01,  3.4432e+00],\n",
            "          [-9.5598e-01, -5.6541e+00, -4.3398e+00,  2.1750e-01,  3.3758e+00,\n",
            "           -6.1343e+00, -4.4104e+00, -6.2147e+00, -7.7017e+00, -1.1325e+00,\n",
            "           -8.7977e-01,  3.4523e+00, -1.1270e+00],\n",
            "          [ 1.0055e+00, -2.8595e+00, -4.5339e-01,  1.1271e+00, -1.2004e+00,\n",
            "            1.6971e+00, -2.8537e+00, -1.4042e+00, -7.0383e-01, -1.2624e+00,\n",
            "            6.3965e-01, -7.5155e-01,  9.4575e-02]]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "через двумерную"
      ],
      "metadata": {
        "id": "-nEUZjRCclOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import conv_transpose2d\n",
        "import pytest\n",
        "import numpy as np\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def TranspConv2d(matrix, in_channels, out_channels, kernel_size, transp_stride=1, padding=0, dilation=1, bias=True, padding_mode='zeros'):\n",
        "    stride = 1\n",
        "\n",
        "    #добавление отступов и padding в входной матрице\n",
        "    pad = kernel_size - 1\n",
        "    result_matrix = []\n",
        "    for matr in matrix:\n",
        "      zero_tensor = np.zeros((((matr.shape[0]-1)*(transp_stride)+1), ((matr.shape[1]-1)*(transp_stride)+1)))\n",
        "      for a in range (0, zero_tensor.shape[0], transp_stride):\n",
        "        for b in range (0, zero_tensor.shape[1], transp_stride):\n",
        "          zero_tensor[a][b] = matr[a//(transp_stride)][b//(transp_stride)]\n",
        "\n",
        "      pad_matr = np.pad(zero_tensor, pad_width=pad, mode='constant')\n",
        "      result_matrix.append(pad_matr)\n",
        "    matrix = torch.tensor(result_matrix)\n",
        "\n",
        "    #генерация bias\n",
        "    if bias == True:\n",
        "      bias_val = torch.rand(out_channels)\n",
        "    else:\n",
        "      bias_val = torch.zeros(out_channels)\n",
        "\n",
        "    #padding_mode\n",
        "    if (padding_mode == 'zeros'):\n",
        "      pad = torch.nn.ZeroPad2d(padding)\n",
        "      matrix = pad(matrix)\n",
        "    if (padding_mode == 'reflect'):\n",
        "      pad = torch.nn.ReflectionPad2d(padding)\n",
        "      matrix = pad(matrix)\n",
        "    if (padding_mode == 'replicate'):\n",
        "      pad = torch.nn.ReplicationPad2d(padding)\n",
        "      matrix = pad(matrix)\n",
        "    if (padding_mode == 'circular'):\n",
        "      pad = torch.nn.CircularPad2d(padding)\n",
        "      matrix = pad(matrix)\n",
        "\n",
        "    #генерация ядра\n",
        "    filter = np.array(torch.rand(out_channels, in_channels, kernel_size, kernel_size))\n",
        "\n",
        "    #инвертирование ядра для ConvTranspose2d\n",
        "    filter_for_transpose = []\n",
        "    for j in range(out_channels):\n",
        "      filter_in = []\n",
        "      for i in range(in_channels):\n",
        "        filter_in.append(np.flip(np.array(filter[j][i])))\n",
        "      filter_for_transpose.append(filter_in)\n",
        "\n",
        "    filter_for_transpose = torch.tensor(filter_for_transpose)\n",
        "    filter_for_transpose = filter_for_transpose.reshape(in_channels, out_channels, kernel_size, kernel_size)\n",
        "\n",
        "\n",
        "\n",
        "    result = []\n",
        "    for l in range(out_channels):\n",
        "      feature_map = np.array([]) #генерация пустой feature-map\n",
        "      for i in range (0, matrix.shape[1]-((filter.shape[2]-1)*dilation+1)+1, stride): #(filter.size - 1)*dilation + 1 при delation\n",
        "        for j in range (0, matrix.shape[2]-((filter.shape[3]-1)*dilation+1)+1, stride):\n",
        "          summa = 0\n",
        "          for c in range (in_channels):\n",
        "            val = matrix[c][i:i+(filter.shape[2]-1)*dilation+1:dilation, j:j+(filter.shape[3]-1)*dilation+1:dilation]\n",
        "            mini_sum = (val*filter[l][c]).sum()\n",
        "            summa = summa + mini_sum\n",
        "          feature_map = np.append(feature_map, float(summa + bias_val[l])) #bias\n",
        "      result.append(feature_map.reshape((matrix.shape[1]-((filter.shape[2]-1)*dilation+1))//stride+1, (matrix.shape[2]-((filter.shape[3]-1)*dilation+1))//stride+1))\n",
        "\n",
        "\n",
        "    return np.array(result), torch.tensor(np.array(filter_for_transpose)), torch.tensor(np.array(bias_val))\n",
        "\n",
        "\n",
        "\n",
        "tensor1 = torch.rand(3, 7, 10)\n",
        "\n",
        "result, kernel, bias_val = TranspConv2d(tensor1, in_channels=3, out_channels=1, kernel_size=3, transp_stride=2, bias=True,)\n",
        "torchFunction = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=3, stride=2, bias=True,)\n",
        "torchFunction.weight.data = kernel\n",
        "torchFunction.bias.data = bias_val\n",
        "\n",
        "print(np.array_equal(torchFunction(tensor1).detach().numpy().astype(\"float16\"), result.astype(\"float16\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cndtsXvSnWR1",
        "outputId": "0121c450-c75c-49fb-d33b-9d92ca1298c7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.rand(3, 4, 4)\n",
        "\n",
        "result, kernel, bias_val = TranspConv2d(tensor1, in_channels=3, out_channels=1, kernel_size=3, transp_stride=2, bias=True,)\n",
        "torchFunction = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=3, stride=2, bias=True,)\n",
        "torchFunction.weight.data = kernel\n",
        "torchFunction.bias.data = bias_val\n",
        "\n",
        "print(np.array_equal(torchFunction(tensor1).detach().numpy().astype(\"float16\"), result.astype(\"float16\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ztRE4BFvzjz",
        "outputId": "aaefb7e2-c2f5-46ed-a7f8-3a85161625d4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.rand(3, 7, 9)\n",
        "\n",
        "result, kernel, bias_val = TranspConv2d(tensor1, in_channels=3, out_channels=1, kernel_size=3, transp_stride=2, bias=True,)\n",
        "torchFunction = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=3, stride=2, bias=True,)\n",
        "torchFunction.weight.data = kernel\n",
        "torchFunction.bias.data = bias_val\n",
        "\n",
        "print(np.array_equal(torchFunction(tensor1).detach().numpy().astype(\"float16\"), result.astype(\"float16\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-J42qUbjW-z",
        "outputId": "6f8c6d30-f054-477d-f178-eecf4f044719"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.rand(3, 16, 12)\n",
        "\n",
        "result, kernel, bias_val = TranspConv2d(tensor1, in_channels=3, out_channels=1, kernel_size=3, transp_stride=2, bias=True,)\n",
        "torchFunction = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=3, stride=2, bias=True,)\n",
        "torchFunction.weight.data = kernel\n",
        "torchFunction.bias.data = bias_val\n",
        "\n",
        "print(np.array_equal(torchFunction(tensor1).detach().numpy().astype(\"float16\"), result.astype(\"float16\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMnn-5sPjZni",
        "outputId": "fb019e7c-8107-41e2-b347-9531ad085a58"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.rand(3, 2, 3)\n",
        "\n",
        "result, kernel, bias_val = TranspConv2d(tensor1, in_channels=3, out_channels=1, kernel_size=3, transp_stride=2, bias=True,)\n",
        "torchFunction = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=3, stride=2, bias=True,)\n",
        "torchFunction.weight.data = kernel\n",
        "torchFunction.bias.data = bias_val\n",
        "\n",
        "print(np.array_equal(torchFunction(tensor1).detach().numpy().astype(\"float16\"), result.astype(\"float16\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y99HpCV7jdW6",
        "outputId": "85b70d76-bb04-4112-ef18-4ab7ae7d8ea1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = torch.rand(3, 2, 3)\n",
        "\n",
        "result, kernel, bias_val = TranspConv2d(tensor1, in_channels=3, out_channels=1, kernel_size=3, transp_stride=2, bias=True,)\n",
        "torchFunction = torch.nn.ConvTranspose2d(in_channels=3, out_channels=1, kernel_size=3, stride=2, bias=True,)\n",
        "torchFunction.weight.data = kernel\n",
        "torchFunction.bias.data = bias_val\n",
        "\n",
        "print(\"Результат выполнения моей функции TranspConv2d:\")\n",
        "print(torchFunction(tensor1).detach().numpy().astype(\"float16\"))\n",
        "print()\n",
        "print(\"Результат выполнения встроенной функции conv_transpose2d из библиотеки PyTorch:\")\n",
        "print(result.astype(\"float16\"))\n",
        "print(np.array_equal(torchFunction(tensor1).detach().numpy().astype(\"float16\"), result.astype(\"float16\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN0RqNejkJkL",
        "outputId": "a61b052a-aefe-48f5-eeab-44c0e02aa6cb"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результат выполнения моей функции TranspConv2d:\n",
            "[[[0.9375 0.893  1.404  0.804  1.867  1.137  1.292 ]\n",
            "  [0.8467 0.8027 1.222  0.7607 1.964  1.0625 1.254 ]\n",
            "  [1.891  1.972  3.004  1.779  2.703  1.907  1.648 ]\n",
            "  [1.587  1.121  1.672  0.74   0.954  0.6567 0.6445]\n",
            "  [1.479  1.873  1.905  0.7817 1.089  1.2    1.067 ]]]\n",
            "\n",
            "Результат выполнения встроенной функции conv_transpose2d из библиотеки PyTorch:\n",
            "[[[0.9375 0.893  1.404  0.804  1.867  1.137  1.292 ]\n",
            "  [0.8467 0.8027 1.222  0.7607 1.964  1.0625 1.254 ]\n",
            "  [1.891  1.972  3.004  1.779  2.703  1.907  1.648 ]\n",
            "  [1.587  1.121  1.672  0.74   0.954  0.6567 0.6445]\n",
            "  [1.479  1.873  1.905  0.7817 1.089  1.2    1.067 ]]]\n",
            "True\n"
          ]
        }
      ]
    }
  ]
}